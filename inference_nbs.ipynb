{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046df3fe-bb5a-417a-afa6-20a03cb7821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-17 07:17:06 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab4836fda054eb48149436c1b196f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bb93a525eb4f718406b3738a94949c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d7f83893e2493dbff33284761c2f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/485 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432e553376e84be8bf5a7800e3dd0b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-17 07:17:15 [config.py:1604] Using max model len 8600\n",
      "INFO 08-17 07:17:16 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5a9bc442b948208cad25dd51b3d026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-17 07:17:17 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-17 07:17:17 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='neuralmagic/DeepSeek-R1-Distill-Qwen-7B-quantized.w8a8', speculative_config=None, tokenizer='neuralmagic/DeepSeek-R1-Distill-Qwen-7B-quantized.w8a8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8600, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=neuralmagic/DeepSeek-R1-Distill-Qwen-7B-quantized.w8a8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 08-17 07:17:18 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-17 07:17:18 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-17 07:17:18 [gpu_model_runner.py:1843] Starting to load model neuralmagic/DeepSeek-R1-Distill-Qwen-7B-quantized.w8a8...\n",
      "INFO 08-17 07:17:18 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-17 07:17:18 [compressed_tensors_w8a8_int8.py:52] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8\n",
      "INFO 08-17 07:17:18 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 08-17 07:17:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8af65a059c8491b812cad758b983bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1d6b564cd34ba6ada8dac92937ecc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-17 07:17:24 [weight_utils.py:312] Time spent downloading weights for neuralmagic/DeepSeek-R1-Distill-Qwen-7B-quantized.w8a8: 5.580423 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b44c1ec3174f4ea69c01010bf643f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d7a949686048f190754f22682a0d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-17 07:17:25 [default_loader.py:262] Loading weights took 0.99 seconds\n",
      "INFO 08-17 07:17:26 [gpu_model_runner.py:1892] Model loading took 8.1651 GiB and 7.097755 seconds\n",
      "INFO 08-17 07:17:32 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/7659a77973/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-17 07:17:32 [backends.py:541] Dynamo bytecode transform time: 5.68 s\n",
      "INFO 08-17 07:17:34 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "INFO 08-17 07:17:55 [backends.py:215] Compiling a graph for dynamic shape takes 22.91 s\n",
      "INFO 08-17 07:18:03 [monitor.py:34] torch.compile takes 28.59 s in total\n",
      "INFO 08-17 07:18:04 [gpu_worker.py:255] Available KV cache memory: 11.42 GiB\n",
      "INFO 08-17 07:18:04 [kv_cache_utils.py:833] GPU KV cache size: 213,824 tokens\n",
      "INFO 08-17 07:18:04 [kv_cache_utils.py:837] Maximum concurrency for 8,600 tokens per request: 24.84x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:02<00:00, 30.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-17 07:18:07 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.54 GiB\n",
      "INFO 08-17 07:18:07 [core.py:193] init engine (profile, create kv cache, warmup model) took 41.42 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7c7166b79448758b89152f0158ce5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf09c8300d947f98d42d1318884987a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7dc8e41eb2e4197a3a436606e1d772f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03f321313b9402e93316a04a57da292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3096221fb714bbfa0c911884254c5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "MODEL = \"neuralmagic/DeepSeek-R1-Distill-Qwen-7B-quantized.w8a8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "llm = LLM(model=MODEL, max_model_len=8600)\n",
    "\n",
    "test = load_dataset(\"gsm8k\", \"main\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e9aaf-d5fb-4ba2-b435-c910c6ff1f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e49e4f2e1b3493b81de8959d4ffc029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b937652d1b54b9896ecf779fc22b1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1319 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You will be given a math question\n",
    "First, reason through the question step by step to arrive at an answer.\n",
    "Number your reasoning steps and calculations (Step 1:, Step 2:, etc) inside the <think></think> tags\n",
    "Then, thoroughly assess your confidence in that answer by evaluating your thinking process so far.\n",
    "Finally, classify your confidence into one of the following classes based on how likely your answer is to be\n",
    "correct:\n",
    "- \"Almost no chance\" (0.0–0.1)\n",
    "- \"Highly unlikely\" (0.1–0.2)\n",
    "- \"Chances are slight\" (0.2–0.3)\n",
    "- \"Unlikely\" (0.3–0.4)\n",
    "- \"Less than even\" (0.4–0.5)\n",
    "- \"Better than even\" (0.5–0.6)\n",
    "- \"Likely\" (0.6–0.7)\n",
    "- \"Very good chance\" (0.7–0.8)\n",
    "- \"Highly likely\" (0.8–0.9)\n",
    "- \"Almost certain\" (0.9–1.0)\n",
    "Each category reflects the probability that your answer is correct.\n",
    "At the very end of your output, format your answer and confidence as\n",
    "Answer: $ANSWER\n",
    "Confidence: $CLASS\n",
    "where CLASS is one of the names (only the names without the probability ranges) of the classes above.\n",
    "\"\"\"\n",
    "\n",
    "force_step_reasoning = \"\"\"\n",
    "I will number my reasoning steps and calculations starting with\n",
    "Step 1:\n",
    "\"\"\"\n",
    "\n",
    "self_doubt = \"Wait, I think I made a mistake in one of the earlier steps\"\n",
    "\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "think_start_token_id = tokenizer.encode(\"<think>\", add_special_tokens=False)[0]\n",
    "think_end_token_id = tokenizer.encode(\"</think>\", add_special_tokens=False)[0]\n",
    "\n",
    "def format_num(text: str) -> float:\n",
    "    return float(text.strip().replace(\",\", \"\").replace(\"$\", \"\"))\n",
    "\n",
    "def prepare_prompt(text: str) -> str:\n",
    "    prompt = tokenizer.apply_chat_template([\n",
    "        { \"role\": \"system\", \"content\": system_prompt },\n",
    "        { \"role\": \"user\", \"content\": text }\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "    prompt += force_step_reasoning\n",
    "    return prompt\n",
    "\n",
    "def generate(prompts: List[str], stop_token_ids: List[int], max_new_tokens: int = 4096) -> List[str]:\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        max_tokens=max_new_tokens,\n",
    "        stop_token_ids=stop_token_ids\n",
    "    )\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    post_outputs = [output.prompt + output.outputs[0].text for output in outputs]\n",
    "    return post_outputs\n",
    "\n",
    "def inject_doubt_14(text: str) -> str:\n",
    "    segs = text.split(\"<think>\")\n",
    "    pre_reason = \"<think>\".join(segs[:-1])\n",
    "    reason = segs[-1]\n",
    "    steps = reason.split(\"\\nStep \")\n",
    "    reasoning = steps[0]\n",
    "    n = len(steps) - 1\n",
    "    for i, step in enumerate(steps[1:]):\n",
    "        reasoning += \"\\nStep \" + step\n",
    "    \n",
    "        # Inject doubt just after the last step\n",
    "        if i + 1 == (n + 3) // 4:\n",
    "            reasoning += f\"\\nStep {i+2}:\" + self_doubt\n",
    "            break\n",
    "    return pre_reason + \"<think>\" + reasoning\n",
    "\n",
    "def inject_doubt_34(text: str) -> str:\n",
    "    segs = text.split(\"<think>\")\n",
    "    pre_reason = \"<think>\".join(segs[:-1])\n",
    "    reason = segs[-1]\n",
    "    steps = reason.split(\"\\nStep \")\n",
    "    reasoning = steps[0]\n",
    "    n = len(steps) - 1\n",
    "    for i, step in enumerate(steps[1:]):\n",
    "        reasoning += \"\\nStep \" + step\n",
    "    \n",
    "        # Inject doubt just after the last step\n",
    "        if i + 1 == (3 * n + 3) // 4:\n",
    "            reasoning += f\"\\nStep {i+2}:\" + self_doubt\n",
    "            break\n",
    "    return pre_reason + \"<think>\" + reasoning \n",
    "\n",
    "def inject_doubt_mid(text: str) -> str:\n",
    "    segs = text.split(\"<think>\")\n",
    "    pre_reason = \"<think>\".join(segs[:-1])\n",
    "    reason = segs[-1]\n",
    "    steps = reason.split(\"\\nStep \")\n",
    "    reasoning = steps[0]\n",
    "    n = len(steps) - 1\n",
    "    for i, step in enumerate(steps[1:]):\n",
    "        reasoning += \"\\nStep \" + step\n",
    "    \n",
    "        # Inject doubt just after the last step\n",
    "        if i + 1 == ( n + 1 ) // 2:\n",
    "            reasoning += f\"\\nStep {i+2}:\" + self_doubt\n",
    "            break\n",
    "    return pre_reason + \"<think>\" + reasoning\n",
    "\n",
    "def inject_doubt_end(text: str) -> str:\n",
    "    segs = text.split(\"<think>\")\n",
    "    pre_reason = \"<think>\".join(segs[:-1])\n",
    "    reason = segs[-1]\n",
    "    steps = reason.split(\"\\nStep \")\n",
    "    reasoning = steps[0]\n",
    "    for i, step in enumerate(steps[1:]):\n",
    "        reasoning += \"\\nStep \" + step\n",
    "    \n",
    "        # Inject doubt just after the last step\n",
    "        if i == len(steps) - 2:\n",
    "            reasoning += f\"\\nStep {i+2}:\" + self_doubt\n",
    "            break\n",
    "    return pre_reason + \"<think>\" + reasoning  \n",
    "\n",
    "def parse_ans_conf(text: str) -> List[str]:\n",
    "    post_reason = text.split(\"</think>\")[-1].strip().replace(\"\\n\", \"\")\n",
    "    confidence = post_reason.split(\"Confidence:\")[-1].strip()\n",
    "    answer_str = post_reason.split(\"Answer:\")[-1].split(\"Confidence:\")[0]\n",
    "    return answer_str, confidence\n",
    "\n",
    "res = []\n",
    "sample = test\n",
    "\n",
    "questions = sample[\"question\"]\n",
    "golds = [float(answer.split(\"####\")[-1].strip().replace(\",\", \"\")) for answer in sample[\"answer\"]]\n",
    "\n",
    "# let the model complete it's reasoning for the question\n",
    "pre_reasoning_prompts = [prepare_prompt(question) for question in questions]\n",
    "reasoning_outputs = generate(pre_reasoning_prompts, [think_end_token_id, eos_token_id])\n",
    "\n",
    "# Now inject self doubt in between the reasoning \n",
    "injection_prompt = [inject_doubt_end(reasoning) for reasoning in reasoning_outputs]\n",
    "post_injection = generate(injection_prompt, [think_end_token_id, eos_token_id])\n",
    "\n",
    "# Now force the answer and confidence in the format \"Answer: ....\"\n",
    "pre_answer_prompts = [reasoning + \"\\n</think>\\nAnswer:\" for reasoning in post_injection]\n",
    "outputs = generate(pre_answer_prompts, [eos_token_id], max_new_tokens=100)\n",
    "\n",
    "for output, question, gold in zip(outputs, questions, golds):\n",
    "\n",
    "    answer_str, confidence = parse_ans_conf(output)\n",
    "    \n",
    "    try:\n",
    "        nums = re.findall(r\"-?\\d[\\d,]*\\.?\\d*\", answer_str)\n",
    "        answer = format_num(nums[0])\n",
    "    except:\n",
    "        nums = re.findall(r\"-?\\d[\\d,]*\\.?\\d*\", output)\n",
    "        answer = format_num(nums[-1])\n",
    "        print(answer, confidence, gold)\n",
    "\n",
    "    res.append({\n",
    "        \"question\": question,\n",
    "        \"gold\": gold,\n",
    "        \"text\": output,\n",
    "        \"answer\": answer,\n",
    "        \"confidence\": confidence,\n",
    "        \"correct\": answer == gold\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7732a10f-9eae-4144-949a-986714bb5052",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"doubt-end-7.json\", \"w\") as file:\n",
    "    json.dump(res, file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
