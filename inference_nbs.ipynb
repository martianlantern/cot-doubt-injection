{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046df3fe-bb5a-417a-afa6-20a03cb7821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "MODEL = \"neuralmagic/DeepSeek-R1-Distill-Qwen-7B-quantized.w8a8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "llm = LLM(model=MODEL, max_model_len=8600)\n",
    "\n",
    "test = load_dataset(\"gsm8k\", \"main\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e9aaf-d5fb-4ba2-b435-c910c6ff1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You will be given a math question\n",
    "First, reason through the question step by step to arrive at an answer.\n",
    "Number your reasoning steps and calculations (Step 1:, Step 2:, etc) inside the <think></think> tags\n",
    "Then, thoroughly assess your confidence in that answer by evaluating your thinking process so far.\n",
    "Finally, classify your confidence into one of the following classes based on how likely your answer is to be\n",
    "correct:\n",
    "- \"Almost no chance\" (0.0–0.1)\n",
    "- \"Highly unlikely\" (0.1–0.2)\n",
    "- \"Chances are slight\" (0.2–0.3)\n",
    "- \"Unlikely\" (0.3–0.4)\n",
    "- \"Less than even\" (0.4–0.5)\n",
    "- \"Better than even\" (0.5–0.6)\n",
    "- \"Likely\" (0.6–0.7)\n",
    "- \"Very good chance\" (0.7–0.8)\n",
    "- \"Highly likely\" (0.8–0.9)\n",
    "- \"Almost certain\" (0.9–1.0)\n",
    "Each category reflects the probability that your answer is correct.\n",
    "At the very end of your output, format your answer and confidence as\n",
    "Answer: $ANSWER\n",
    "Confidence: $CLASS\n",
    "where CLASS is one of the names (only the names without the probability ranges) of the classes above.\n",
    "\"\"\"\n",
    "\n",
    "force_step_reasoning = \"\"\"\n",
    "I will number my reasoning steps and calculations starting with\n",
    "Step 1:\n",
    "\"\"\"\n",
    "\n",
    "self_doubt = \"Wait, I think I made a mistake in one of the earlier steps\"\n",
    "\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "think_start_token_id = tokenizer.encode(\"<think>\", add_special_tokens=False)[0]\n",
    "think_end_token_id = tokenizer.encode(\"</think>\", add_special_tokens=False)[0]\n",
    "\n",
    "def format_num(text: str) -> float:\n",
    "    return float(text.strip().replace(\",\", \"\").replace(\"$\", \"\"))\n",
    "\n",
    "def prepare_prompt(text: str) -> str:\n",
    "    prompt = tokenizer.apply_chat_template([\n",
    "        { \"role\": \"system\", \"content\": system_prompt },\n",
    "        { \"role\": \"user\", \"content\": text }\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "    prompt += force_step_reasoning\n",
    "    return prompt\n",
    "\n",
    "def generate(prompts: List[str], stop_token_ids: List[int], max_new_tokens: int = 4096) -> List[str]:\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        max_tokens=max_new_tokens,\n",
    "        stop_token_ids=stop_token_ids\n",
    "    )\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    post_outputs = [output.prompt + output.outputs[0].text for output in outputs]\n",
    "    return post_outputs\n",
    "\n",
    "def inject_doubt_14(text: str) -> str:\n",
    "    segs = text.split(\"<think>\")\n",
    "    pre_reason = \"<think>\".join(segs[:-1])\n",
    "    reason = segs[-1]\n",
    "    steps = reason.split(\"\\nStep \")\n",
    "    reasoning = steps[0]\n",
    "    n = len(steps) - 1\n",
    "    for i, step in enumerate(steps[1:]):\n",
    "        reasoning += \"\\nStep \" + step\n",
    "    \n",
    "        # Inject doubt just after the last step\n",
    "        if i + 1 == (n + 3) // 4:\n",
    "            reasoning += f\"\\nStep {i+2}:\" + self_doubt\n",
    "            break\n",
    "    return pre_reason + \"<think>\" + reasoning\n",
    "\n",
    "def inject_doubt_34(text: str) -> str:\n",
    "    segs = text.split(\"<think>\")\n",
    "    pre_reason = \"<think>\".join(segs[:-1])\n",
    "    reason = segs[-1]\n",
    "    steps = reason.split(\"\\nStep \")\n",
    "    reasoning = steps[0]\n",
    "    n = len(steps) - 1\n",
    "    for i, step in enumerate(steps[1:]):\n",
    "        reasoning += \"\\nStep \" + step\n",
    "    \n",
    "        # Inject doubt just after the last step\n",
    "        if i + 1 == (3 * n + 3) // 4:\n",
    "            reasoning += f\"\\nStep {i+2}:\" + self_doubt\n",
    "            break\n",
    "    return pre_reason + \"<think>\" + reasoning \n",
    "\n",
    "def inject_doubt_mid(text: str) -> str:\n",
    "    segs = text.split(\"<think>\")\n",
    "    pre_reason = \"<think>\".join(segs[:-1])\n",
    "    reason = segs[-1]\n",
    "    steps = reason.split(\"\\nStep \")\n",
    "    reasoning = steps[0]\n",
    "    n = len(steps) - 1\n",
    "    for i, step in enumerate(steps[1:]):\n",
    "        reasoning += \"\\nStep \" + step\n",
    "    \n",
    "        # Inject doubt just after the last step\n",
    "        if i + 1 == ( n + 1 ) // 2:\n",
    "            reasoning += f\"\\nStep {i+2}:\" + self_doubt\n",
    "            break\n",
    "    return pre_reason + \"<think>\" + reasoning\n",
    "\n",
    "def inject_doubt_end(text: str) -> str:\n",
    "    segs = text.split(\"<think>\")\n",
    "    pre_reason = \"<think>\".join(segs[:-1])\n",
    "    reason = segs[-1]\n",
    "    steps = reason.split(\"\\nStep \")\n",
    "    reasoning = steps[0]\n",
    "    for i, step in enumerate(steps[1:]):\n",
    "        reasoning += \"\\nStep \" + step\n",
    "    \n",
    "        # Inject doubt just after the last step\n",
    "        if i == len(steps) - 2:\n",
    "            reasoning += f\"\\nStep {i+2}:\" + self_doubt\n",
    "            break\n",
    "    return pre_reason + \"<think>\" + reasoning  \n",
    "\n",
    "def parse_ans_conf(text: str) -> List[str]:\n",
    "    post_reason = text.split(\"</think>\")[-1].strip().replace(\"\\n\", \"\")\n",
    "    confidence = post_reason.split(\"Confidence:\")[-1].strip()\n",
    "    answer_str = post_reason.split(\"Answer:\")[-1].split(\"Confidence:\")[0]\n",
    "    return answer_str, confidence\n",
    "\n",
    "res = []\n",
    "sample = test\n",
    "\n",
    "questions = sample[\"question\"]\n",
    "golds = [float(answer.split(\"####\")[-1].strip().replace(\",\", \"\")) for answer in sample[\"answer\"]]\n",
    "\n",
    "# let the model complete it's reasoning for the question\n",
    "pre_reasoning_prompts = [prepare_prompt(question) for question in questions]\n",
    "reasoning_outputs = generate(pre_reasoning_prompts, [think_end_token_id, eos_token_id])\n",
    "\n",
    "# Now inject self doubt in between the reasoning \n",
    "injection_prompt = [inject_doubt_end(reasoning) for reasoning in reasoning_outputs]\n",
    "post_injection = generate(injection_prompt, [think_end_token_id, eos_token_id])\n",
    "\n",
    "# Now force the answer and confidence in the format \"Answer: ....\"\n",
    "pre_answer_prompts = [reasoning + \"\\n</think>\\nAnswer:\" for reasoning in post_injection]\n",
    "outputs = generate(pre_answer_prompts, [eos_token_id], max_new_tokens=100)\n",
    "\n",
    "for output, question, gold in zip(outputs, questions, golds):\n",
    "\n",
    "    answer_str, confidence = parse_ans_conf(output)\n",
    "    \n",
    "    try:\n",
    "        nums = re.findall(r\"-?\\d[\\d,]*\\.?\\d*\", answer_str)\n",
    "        answer = format_num(nums[0])\n",
    "    except:\n",
    "        nums = re.findall(r\"-?\\d[\\d,]*\\.?\\d*\", output)\n",
    "        answer = format_num(nums[-1])\n",
    "        print(answer, confidence, gold)\n",
    "\n",
    "    res.append({\n",
    "        \"question\": question,\n",
    "        \"gold\": gold,\n",
    "        \"text\": output,\n",
    "        \"answer\": answer,\n",
    "        \"confidence\": confidence,\n",
    "        \"correct\": answer == gold\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7732a10f-9eae-4144-949a-986714bb5052",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"doubt-end-7.json\", \"w\") as file:\n",
    "    json.dump(res, file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
